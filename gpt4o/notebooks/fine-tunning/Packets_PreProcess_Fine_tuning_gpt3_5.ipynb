{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2dHCQwHCbHQ",
    "tags": []
   },
   "source": [
    "# Fine-tuning pre-processing\n",
    "-----\n",
    "\n",
    "En este código de ejemplo se van a tratar los siguientes aspectos relevantes para el fine-tunning en este orden:\n",
    "\n",
    "\n",
    "1. **Preparación de los datos**\n",
    "    - **Carga de los datos**\n",
    "    - **Reformateo para que sigan el formato indicado más adelante**\n",
    "2. **Validación de los datos y detección de errores en ellos**\n",
    "    - **Busqueda de errores**\n",
    "    - **Estudio de la distribución de los mensajes**\n",
    "3. **Estimación del coste del entrenamiento**\n",
    "4. **Almacenamiento de la BD**\n",
    "5. **Subida de BD a OpenAI para su posterior uso**\n",
    "    - **Crear un trabajo de fine-tunning**\n",
    "6. **Como usar un modelo de Fine-tunning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jzgtC7fsDlhv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'Fill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1s1OPq-uGk_L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pkl_saved = \"../../data/Packets/ARP/pickle/ARP_packet_pairs_Aday_validation_v2.pkl\"\n",
    "file_path = 'DBs/ARP/ARP_DB_validation_v2.jsonl'\n",
    "\n",
    "system_message = 'You are a new generation traffic generator. \\\n",
    "You are specilized in the ARP protocol and traffic generation using python and scapy. \\\n",
    "You are especially attentive to variables and different types of traffic.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9mqJttkXH6K"
   },
   "source": [
    "## **Step 1: Prepare your data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0rlrtoi9tJP"
   },
   "source": [
    "### 1.1 Cargar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6wocRzSFo0y"
   },
   "source": [
    "Cargamos los diálogos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class response():\n",
    "    \"Stores name and place pairs\"\n",
    "    def __init__(self, name, place):\n",
    "        self.prompt_summary = name\n",
    "        self.completion = place\n",
    "\n",
    "responses = pickle.load( open(pkl_saved, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'a4:9b:4f:53:00:0d     a4:9b:4f:53:00:0e     ARP      60     192.168.2.13 is at a4:9b:4f:53:00:0d\\na4:9b:4f:53:00:0e     Broadcast             ARP      60     Gratuitous ARP for 192.168.2.14 (Reply)\\n', 'completion': '\\n\\npkt_list = [\\n    scapy.Ether(src=\"a4:9b:4f:53:00:0d\", dst=\"a4:9b:4f:53:00:0e\")/scapy.ARP(op=2, psrc=\"192.168.2.13\", hwsrc=\"a4:9b:4f:53:00:0d\", hwdst=\"a4:9b:4f:53:00:0e\"),\\n    scapy.Ether(src=\"a4:9b:4f:53:00:0e\", dst=\"FF:FF:FF:FF:FF:FF\")/scapy.ARP(op=2, psrc=\"192.168.2.14\", hwsrc=\"a4:9b:4f:53:00:0e\", hwdst=\"FF:FF:FF:FF:FF:FF\", pdst=\"192.168.2.14\")\\n]'}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for item in responses:\n",
    "    data.append({\"prompt\":item.prompt_summary,\"completion\":item.completion.choices[0].text})\n",
    "    #data[-1][\"prompt\"] += \"\\n###\\n\\n\"\n",
    "    #data[-1][\"completion\"] = \"\\n\" + data[-1][\"completion\"]\n",
    "    #data[-1][\"completion\"] += \"\\n\\n###\"\n",
    "\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Función para dividir prompts múltiples en parejas resumen-comando, haciendo que todas las relaciones sean 1 a 1 y no N a N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Todoen1(data):\n",
    "    DB= []\n",
    "    for conjunto in data:\n",
    "        lines_query =[]\n",
    "        lines_completion = []\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(conjunto)\n",
    "        lines_query = conjunto[\"prompt\"].splitlines()\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(lines_query)\n",
    "        lines_completion = conjunto[\"completion\"].splitlines()\n",
    "        #print(lines_completion)\n",
    "        lines_completion = lines_completion[3:-1]\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(lines_completion)\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(len(lines_query))\n",
    "        for index in range(len(lines_query)-1):\n",
    "            #print(lines_query[index])\n",
    "            #print(lines_completion[index][:-1])\n",
    "            #print(\"-------------------------------------------------------------------------------------\")\n",
    "            #print(\"-------------------------------------------------------------------------------------\")\n",
    "            DB.append({\"prompt\":lines_query[index],\"completion\":lines_completion[index][:-1].strip()})\n",
    "        \n",
    "    return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Función para procesar prompts y dejarlos como están de manera que relación sea N a N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TodoTalCual(data):\n",
    "    DB= []\n",
    "    for conjunto in data:\n",
    "        lines_query =[]\n",
    "        lines_completion = []\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(conjunto)\n",
    "        lines_query = conjunto[\"prompt\"].splitlines()\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(str(lines_query))\n",
    "        lines_completion = conjunto[\"completion\"].splitlines()\n",
    "        #lines_completion = lines_completion[2:-2]\n",
    "        #print(\"-------------------------------------------------------------------------------------\")\n",
    "        print(lines_completion)\n",
    "        print(\"-------------------------------------------------------------------------------------\")\n",
    "        print(\"-------------------------------------------------------------------------------------\")\n",
    "        #print(len(lines_query))\n",
    "        completion =[]\n",
    "        for index in range(len(lines_completion)):\n",
    "            completion.append(lines_completion[index][:-1].strip())\n",
    "        print(completion)\n",
    "        print(\"-----\")\n",
    "        DB.append({\"prompt\":lines_query[index],\"completion\":str(completion)})\n",
    "        \n",
    "    return DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Aplicar formato necesario\n",
    "\n",
    "Tratamiento de los datos de entrada en función de la salida que se quiera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Si se quiere parejas de 1 a 1\n",
    "DB = Todoen1(data)\n",
    "# Si se quiere relaciones N a N\n",
    "#DB = TodoTalCual(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'e4:8d:8c:7b:00:04     e4:8d:8c:7b:00:03     ARP      60     172.16.0.4 is at e4:8d:8c:7b:00:04', 'completion': 'scapy.Ether(src=\"e4:8d:8c:7b:00:04\", dst=\"e4:8d:8c:7b:00:03\")/scapy.ARP(op=2, psrc=\"172.16.0.4\", hwsrc=\"e4:8d:8c:7b:00:04\", hwdst=\"e4:8d:8c:7b:00:03\",  pdst=socket.inet_ntoa(struct.pack(\">I\", random.randint(1, 0xffffffff))))'}\n"
     ]
    }
   ],
   "source": [
    "print(DB[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHJvVUbL-LgN",
    "tags": []
   },
   "source": [
    "### 1.3 Aplicar formato necesario\n",
    "Ahora debemos asegurarnos que cada ejemplo siga el siguiente formato:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are an assistant that occasionally misspells words\" },\n",
    "    { \"role\": \"user\", \"content\": \"Tell me a story.\" },\n",
    "    { \"role\": \"assistant\", \"content\": \"One day a student went to schoool.\" }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5X1rLzeL3qi"
   },
   "source": [
    "Vamos a programar una función que construye cada ejemplo como un diccionario con una única llave `messages` y cuyo valor es el mensaje del sistema, más la conversación entre usuario y asistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatear_datos(data, system_message=None):\n",
    "    messages = []\n",
    "\n",
    "    # Incluir primero el mensaje de sistema\n",
    "    if system_message:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_message\n",
    "        })\n",
    "    # Iterar por la lista de mensajes\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": data[\"prompt\"]\n",
    "    }\n",
    "\n",
    "    #Agregar el mensaje a la lista\n",
    "    messages.append(user_message)\n",
    "    \n",
    "    assistant_message = {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\": data[\"completion\"]\n",
    "    }\n",
    "    \n",
    "    #Agregar el mensaje a la lista\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "    # Crear diccionario final\n",
    "    dict_final = {\n",
    "        \"messages\": messages\n",
    "    }\n",
    "\n",
    "    return dict_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKPmNp-iNGdf"
   },
   "source": [
    "Aplicamos la función a cada ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "z6FhH8jfNMNF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "conjunto = []\n",
    "for conjunto in DB:\n",
    "    ejemplo_formateado = formatear_datos(data=conjunto,\n",
    "                                        system_message=system_message)\n",
    "\n",
    "    dataset.append(ejemplo_formateado)\n",
    "    conjunto = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTuG35uiAlI_"
   },
   "source": [
    "## **Step 2: Validar formato, errores, y distribuciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03MIJKINRWNu"
   },
   "source": [
    "Revisamos si hay errores y estimamos el precio usando la guía [entregada por OpenAI](https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Busqueda de errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWEEVQU3RWBP",
    "outputId": "1ab3b722-3f1b-4fb8-8bf7-81e50496830c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "from collections import defaultdict\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Estudio Distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "0GyCjiRGR_lp"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribución de {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"media / mediana: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AE7N9BEZSfzq",
    "outputId": "ad8e4bba-7089-4937-c010-783447f298bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num de ejemplos sin el system message: 0\n",
      "Num de ejemplos sin el user message: 0\n",
      "\n",
      "#### Distribución de num_mensajes_por_ejemplo:\n",
      "min / max: 3, 3\n",
      "media / mediana: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribución de num_total_tokens_por_ejemplo:\n",
      "min / max: 191, 237\n",
      "media / mediana: 214.82142857142858, 216.0\n",
      "p5 / p95: 193.0, 237.0\n",
      "\n",
      "#### Distribución de num_assistant_tokens_por_ejemplo:\n",
      "min / max: 99, 122\n",
      "media / mediana: 109.35714285714286, 106.0\n",
      "p5 / p95: 102.0, 122.0\n",
      "\n",
      "0 ejemplos que excedan el límite de tokenes de 4096, ellos serán truncados durante el fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num de ejemplos sin el system message:\", n_missing_system)\n",
    "print(\"Num de ejemplos sin el user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_mensajes_por_ejemplo\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_por_ejemplo\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_por_ejemplo\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} ejemplos que excedan el límite de tokenes de 4096, ellos serán truncados durante el fine-tuning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Estimación del coste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCFF4e8LSm5r",
    "outputId": "313bce52-1225-4d0b-b899-1090b6a8e87d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conjunto de datos tiene ~6015 tokens que serán cargados durante el entrenamiento\n",
      "Por defecto, entrenarás para 4 epochs en este conjunto de datos\n",
      "Por defecto, serás cargado con ~24060 tokens\n",
      "Revisa la página para estimar el costo total\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "TARGET_EPOCHS = 4\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"El conjunto de datos tiene ~{n_billing_tokens_in_dataset} tokens que serán cargados durante el entrenamiento\")\n",
    "print(f\"Por defecto, entrenarás para {n_epochs} epochs en este conjunto de datos\")\n",
    "print(f\"Por defecto, serás cargado con ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "print(\"Revisa la página para estimar el costo total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksJ-d1GkAt8L"
   },
   "source": [
    "## **Step 4: Guardar datos fromateados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeDbspgKS177"
   },
   "source": [
    "Guardamos la base de datos en JSONL=JSON Lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "6Cv9JoRIS5Br"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_to_jsonl(dataset, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for ejemplo in dataset:\n",
    "            json_line = json.dumps(ejemplo, ensure_ascii=False)\n",
    "            file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "9qeWEly_lm0p"
   },
   "outputs": [],
   "source": [
    "#Guardar train full\n",
    "save_to_jsonl(dataset, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2L188k1XVpb"
   },
   "source": [
    "## **Step 5: Upload files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6Wnv0sxGfgV"
   },
   "source": [
    "Cargamos la base de datos a OpenAI y luego imprimimos el id de la respuesta de esta solicitd. Hacemos esto porque vamos a necesitar el id posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-M7hHyuyUJcmGX9v1gRAqjoQS', bytes=17405, created_at=1713459522, filename='ARP_DB_validation_v2.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "openai.api_key = 'Fill'\n",
    "client = OpenAI()\n",
    "\n",
    "client.files.create(\n",
    "  file=open(\"DBs/ARP/ARP_DB_training_v2.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "client.files.create(\n",
    "  file=open(\"DBs/ARP/ARP_DB_validation_v2.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqPijI4sA80Z"
   },
   "source": [
    "### 5.1 Create a fine-tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fkXQAROG_Fd"
   },
   "source": [
    "Luego creamos un punto de trabajo para hacer fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-J22nrKS4x4DfCLRxTdJbNO2m\n"
     ]
    }
   ],
   "source": [
    "protocol = \"ARPv2\"\n",
    "\n",
    "import json\n",
    "\n",
    "with open('DBs/OpenAI_DBs_IDs.jsonl', 'r') as json_file:\n",
    "    IDs_list = json.load(json_file)\n",
    "\n",
    "print(IDs_list[\"Training\"][protocol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.fine_tuning.jobs.create(training_file=IDs_list[\"Training\"][protocol],\n",
    "                                          model=\"gpt-3.5-turbo-1106\", \n",
    "                                          suffix=protocol,\n",
    "                                          hyperparameters={'n_epochs':4},\n",
    "                                          validation_file=IDs_list[\"Validation\"][protocol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "tdjRnPe7falt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-ZdZGVl34NgHQSlzGXz0UzSwY', created_at=1713459610, error=Error(code=None, message=None, param=None, error=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=4, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-1106', object='fine_tuning.job', organization_id='org-ULHsc1YYFDmMSDkN1nMjJgTG', result_files=[], status='validating_files', trained_tokens=None, training_file='file-J22nrKS4x4DfCLRxTdJbNO2m', validation_file='file-M7hHyuyUJcmGX9v1gRAqjoQS', user_provided_suffix='ARPv1', seed=1520269638, integrations=[])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "0LvqXi-t2pud"
   },
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.FineTuningJob, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m openai\u001b[38;5;241m.\u001b[39mFineTuningJob\u001b[38;5;241m.\u001b[39mretrieve(response\u001b[38;5;241m.\u001b[39mid)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PAC-GPT\\Lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.FineTuningJob, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "openai.FineTuningJob.retrieve(response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwSDEq8jnyEF"
   },
   "outputs": [],
   "source": [
    "response = openai.FineTuningJob.list_events(id=response.id)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp-WSUAABDrs"
   },
   "source": [
    "## **Step 6: Use a fine-tuned model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6qWC4gKHMw4"
   },
   "source": [
    "Esperamos a que llegue el correo de confirmación, donde nos entregarán el id del nuevo modelo entrenado. Usaremos langchain (revisa aquí el último tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4MsoLHFHORf"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "model_name = \"ft:gpt-3.5-turbo-0613:evo-academy:burro-shrek:7tg5aZZV\"\n",
    "chat = ChatOpenAI(model=model_name, temperature=0.0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=\"Hola! Soy Jonathan, tanto tiempo que no hablamos. Qué tal tu día?\")\n",
    "]\n",
    "\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUVQ-76bhdi1",
    "outputId": "9215e5ee-8a98-4e67-fcac-eaf75f124aca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola Jonathan! ¡Mucho gusto verte de nuevo! Mi día ha sido bastante interesante, he estado aquí, charlando y respondiendo preguntas. ¿Y tú, cómo ha sido tu día? [levanto una oreja con curiosidad]\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=\"Hola! Soy Jonathan, tanto tiempo que no hablamos. Qué tal tu día?\")\n",
    "]\n",
    "\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Huxt8ceEW88y"
   },
   "source": [
    "Síguenos en nuestras redes:\n",
    "- TikTok: https://www.tiktok.com/@evoacdm\n",
    "- Instagram: https://www.instagram.com/evoacdm/\n",
    "- LinkedIn: https://www.linkedin.com/company/evoacmd/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
