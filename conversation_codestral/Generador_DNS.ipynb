{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbffd3e-abdf-46a3-9142-3b39b15b7e7c",
   "metadata": {},
   "source": [
    "# Creación de conversaciones DNS\n",
    "\r\n",
    "Con este código se va a intentar crear todos los pasos de una conversación DNS, mediante IA generativa. Se pretende pasar una descripción de esta conversación al modelo y que este sea capaz de crear todos los pasos/paquetes necesarios para replicar una conversación con esas mismas especificacioness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b862b41-f9cb-4c58-a5e0-4e68f10aaf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoConfig,\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  GenerationConfig,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb10078-9fd0-41dd-aa72-6d3e2c28b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ac10b2-f517-4c96-b0fd-e52097e334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f04492a-1931-43cb-a154-e0bef3edd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15582f4b-ed2e-41d8-bcff-fce1abafe5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Codestral-22B-v0.1' #'mistralai/Mixtral-8x7B-Instruct-v0.1' #'mistralai/Codestral-22B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdad52f1-9b4e-42c5-8666-a36576106d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55910c2066c4888afb3a1082582c152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, legacy=True)\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_k = 10\n",
    "generation_config.top_p = 0.1\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "    \n",
    "# Crear LLM Chain\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=False,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61011b2-b3a9-4e72-8352-5e2d6566b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"system\", \"user\"],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template = \"\"\"\n",
    "    [INST]\n",
    "    {system}\n",
    "    \n",
    "    {user}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "system_message = \"You are a computer network programmer. Your goal is to generate Python code to create packages with the scapy framework based on a provided conversation summary.\\n\"\n",
    "system_message = \"For example, given the following summary of the DNS conversation:\\n\"\n",
    "system_message += 'Source: IP=\"192.168.1.10\" // Destination: IP=\"192.168.1.20\"// Others: id=0x0045 , resource= \"www.github.com\" , response = \"140.82.112.4\"\\n'\n",
    "system_message += \"The code generated to create a random port number, and the request and reply packets of the provided conversation, must include the following piece of code:\\n\"\n",
    "system_message += \"import... \\n\"\n",
    "system_message += \"... \\n\"\n",
    "system_message += \"RANDOMPORT = random.randint(4097, 65530)\\n\"\n",
    "system_message += \"# Create request packet\\n\"\n",
    "system_message += 'requestpkt = IP(src=\"192.168.1.10\", dst=\"192.168.1.20\", proto=17)/UDP(sport=RANDOMPORT, dport=53)/DNS(id=0x0045, qr=0, rd=1, opcode=0, qdcount=1, ancount=0, nscount=0, arcount=0, qd=DNSQR(qname=\"www.github.com\", qtype=\"A\", qclass=\"IN\"))\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))'\n",
    "system_message += \"# Create reply packet\"\n",
    "system_message += 'replypkt = IP(src=\"192.168.1.20\", dst=\"192.168.1.10\", proto=17)/UDP(sport=53, dport=RANDOMPORT)/DNS(id=0x0aba, qr=1, opcode=0, ra=1, rcode=0, qdcount=1, ancount=1, nscount=0, arcount=0, qd=DNSQR(qname=\"www.github.com\", qtype=\"A\", qclass=\"IN\"), an=DNSRR(rrname=\"www.github.com\", type=\"A\", rclass=\"IN\", ttl=255, rdata=\"140.82.112.4\"))\\n'\n",
    "system_message += \"# Add packets to list\\n\"\n",
    "system_message += \"pktlist = [requestpkt, replypkt]\"   \n",
    "system_message += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dd06e9-27fb-488f-a1e8-f161553cf326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de resúmenes: 60\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/Conversations/DNS/Conv_summaries.txt\",\"r\",encoding=\"utf8\") as f:\n",
    "    Conv_summaries = f.read().splitlines()\n",
    "\n",
    "print(\"Número de resúmenes: \" +str(len(Conv_summaries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100fea14-4b67-461c-aa8d-b55967a439bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class code_response():\n",
    "    \"Stores name and place pairs\"\n",
    "    def __init__(self, name, place):\n",
    "        self.prompt_summary = name\n",
    "        self.completion = place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0bc510-5c51-420f-b1cd-3cffab4f9848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previous responses: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    responses = pickle.load(open(\"./data/Conversations/DNS/pickle/DNS_pairs_Aday.pkl\", \"rb\" ))\n",
    "except:\n",
    "    responses = []\n",
    "    \n",
    "print(\"Number of previous responses: \" + str(len(responses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb631ea6-9b5d-4807-860e-3c2a804f0f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivangm/pytorch_env/pytorch_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "rag_chain = prompt | mistral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552ba376-e01b-4c6f-abba-8045236cf244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5648bdb4316a4cbc840c27d54abdccbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n",
      "Error en la ejecución\n",
      "\n",
      "Number of completitions done: 21\n"
     ]
    }
   ],
   "source": [
    "from scapy.all import *\n",
    "from scapy.utils import RawPcapReader, wrpcap\n",
    "import scapy.all as scapy\n",
    "\n",
    "from scapy.layers.inet import IP\n",
    "from scapy.all import DNS\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(len(Conv_summaries)))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(Conv_summaries)):\n",
    "\n",
    "    text_sum = \"\"\n",
    "    \n",
    "    summary = Conv_summaries[i]\n",
    "    \n",
    "    #print(\"\\n...........................................\")\n",
    "    #print(\"GENERATING SCAPY COMMANDS FOR THE FOLLOWING CONVERSATIONS (iter \" +str(i+1) +\")\\n\")\n",
    "    \n",
    "    query_content = \"Given the following summary of a conversation:\\n\"\n",
    "    query_content += summary + \"\\n\"\n",
    "    text_sum += summary + \"\\n\"\n",
    "    query_content += \"# Instructions:\\n\"\n",
    "    query_content += \"## Generate a complete python code for creating packets with scapy framework. Use the provided example structure to simulate real-world conditions, adding the sleep function call between packets.\\n\"\n",
    "    query_content += \"## Take your time (a few seconds) to validate if the last code line of your code is 'pktlist = [requestpkt, replypkt]', and if the addresses used in the packets correspond to the same ones as in the DNS conversation provided; If not, make the appropriate corrections to the code.\\n\"\n",
    "    query_content += \"## Don't explain the code, just generate the code block itself. PLEASE DONT start the responses with ```python. This is a flagrant error and will make the code unexecutable. You have done this last step wrong plenty of times, take your time to generate the output without ```python please.\\n\"\n",
    "\n",
    "    # Query es el mensaje que le envias al modelo\n",
    "    #print(query_content)\n",
    "\n",
    "    completion = \"\"\n",
    "    completion = rag_chain.invoke({\"system\": system_message, \"user\": query_content})\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    #print(\"\\n...Generated!\")\n",
    "\n",
    "    # Filtramos ```python\n",
    "    #pos1 = completion.find(\"```python\\n\")\n",
    "    #pos2 = completion.find(\"\\n```\")\n",
    "    #print(pos1, pos2)\n",
    "    #if pos1 != -1 and pos2 != 1:\n",
    "    #    completion = completion[pos1+len(\"```python\\n\"):pos2]\n",
    "\n",
    "    # Parece que hay problemas de indentado en el código resultante ¿?\n",
    "    completion_copy = completion\n",
    "    completion = \"\"\n",
    "    for line in completion_copy.splitlines():\n",
    "        completion += line.strip()+\"\\n\"\n",
    "        \n",
    "    # Guardamos tanto la petición como la respuesta en un archivo\n",
    "    #pickle.dump(responses, open( \"./data/Conversations/DNS/pickle/DNS_pairs_Aday.pkl\", \"wb\" ) )\n",
    "    try:\n",
    "        #print(\"Ejecutando python...\")\n",
    "        exec(completion)\n",
    "        #print(pktlist)\n",
    "        counter += 1\n",
    "    except:\n",
    "        print(\"Error en la ejecución\\n\")\n",
    "        #print(completion)\n",
    "        #print(\":\".join(\"{:02x}\".format(ord(c)) for c in completion))\n",
    "        continue\n",
    "\n",
    "    # Guardamos tanto el resumen de los paquetes que solicitamos y lo que devuelve el modelo para estos paquetes\n",
    "    try:\n",
    "        responses.append(code_response(text_sum, completion))\n",
    "    except:\n",
    "        print(\"Error almacenando resultado del modelo\\n\")\n",
    "    \n",
    "    # Guardamos los paquetes generados en un pcap\n",
    "    with open(\"./data/Conversations/DNS/pcap/DNS_generated_codestral_nocuantizado.pcap\", \"ba+\") as f:\n",
    "        wrpcap(f, pktlist, append=True)\n",
    "\n",
    "print(\"Number of completitions done: \" + str(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4c76d1-7ee0-4da1-96e9-04c05591b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: IP= \"198.51.100.1\" // Destination: IP= \"198.51.100.2\" // Others: id=0x1455 , resource= www.hbo.com , response = \"34.192.0.174\"\n",
      "\n",
      "\n",
      "RANDOMPORT = random.randint(4097, 65530)\n",
      "# Create request packet\n",
      "requestpkt = IP(src=\"198.51.100.1\", dst=\"198.51.100.2\", proto=17)/UDP(sport=RANDOMPORT, dport=53)/DNS(id=0x1455, qr=0, rd=1, opcode=0, qdcount=1, ancount=0, nscount=0, arcount=0, qd=DNSQR(qname=\"www.hbo.com\", qtype=\"A\", qclass=\"IN\"))\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "# Create reply packet\n",
      "replypkt = IP(src=\"198.51.100.2\", dst=\"198.51.100.1\", proto=17)/UDP(sport=53, dport=RANDOMPORT)/DNS(id=0x1455, qr=1, opcode=0, ra=1, rcode=0, qdcount=1, ancount=1, nscount=0, arcount=0, qd=DNSQR(qname=\"www.hbo.com\", qtype=\"A\", qclass=\"IN\"), an=DNSRR(rrname=\"www.hbo.com\", type=\"A\", rclass=\"IN\", ttl=255, rdata=\"34.192.0.174\"))\n",
      "# Add packets to list\n",
      "pktlist = [requestpkt, replypkt]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Este es el resumen de lo que está en el struct de responses para la última petición\n",
    "print(responses[-1].prompt_summary)\n",
    "print(responses[-1].completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d67ccd-fac2-4a47-9d5d-f2668310cef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
