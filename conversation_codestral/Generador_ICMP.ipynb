{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbffd3e-abdf-46a3-9142-3b39b15b7e7c",
   "metadata": {},
   "source": [
    "# Creación de conversaciones ICMP\n",
    "\r",
    "Con este código se va a intentar crear todos los pasos de una conversación ICMP, mediante IA generativa. Se pretende pasar una descripción de esta conversación al modelo y que este sea capaz de crear todos los pasos/paquetes necesarios para replicar una conversación con esas mismas especificacioness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b862b41-f9cb-4c58-a5e0-4e68f10aaf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoConfig,\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  GenerationConfig,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb10078-9fd0-41dd-aa72-6d3e2c28b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ac10b2-f517-4c96-b0fd-e52097e334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f04492a-1931-43cb-a154-e0bef3edd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15582f4b-ed2e-41d8-bcff-fce1abafe5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1' #'mistralai/Mixtral-8x7B-Instruct-v0.1' #'mistralai/Codestral-22B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdad52f1-9b4e-42c5-8666-a36576106d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2bd95fabfe4856accb5e65e40ac85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, legacy=True)\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    #device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_k = 10\n",
    "generation_config.top_p = 0.1\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "    \n",
    "# Crear LLM Chain\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=False,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61011b2-b3a9-4e72-8352-5e2d6566b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"system\", \"user\"],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template = \"\"\"\n",
    "    [INST]\n",
    "    {system}\n",
    "    \n",
    "    {user}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "system_message = \"This is an ICMP Echo conversation summary:\\n\"\n",
    "system_message += 'Source: IP=\"192.168.1.10\" // Destination: IP=\"192.168.1.20\"// Others: id=0x0045 seq= 81 ttl=56 type=Echo\"\\n'\n",
    "system_message += \"This is the python code to generate the request and reply packets of the conversation, respectively:\\n\"\n",
    "system_message += 'pkt1=scapy.IP(src=\"192.168.1.10\", dst=\"192.168.1.20\", ttl=56)/scapy.ICMP(type=8, id=0x0045, seq=81)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))'\n",
    "system_message += 'pkt2=scapy.IP(src=\"192.168.1.20\", dst=\"192.168.1.10\", ttl=56)/scapy.ICMP(type=0, id=0x0045, seq=81)\\n'\n",
    "\n",
    "system_message = \"This is an ICMP Timestamp conversation summary:\\n\"\n",
    "system_message += 'Source: IP=\"222.240.4.204\" // Destination: IP=\"192.168.1.20\"// Others: id=0xFF62 seq= 4311 ttl=56 type=Timestamp\"\\n'\n",
    "system_message += \"This is the python code to generate the request and reply packets of the conversation, respectively:\\n\"\n",
    "system_message += 'pkt1=scapy.IP(src=\"222.240.4.204\", dst=\"200.40.184.21\", ttl=56)/scapy.ICMP(type=13, id=0xFF62, seq=4311)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))'\n",
    "system_message += 'pkt2=scapy.IP(src=\"200.40.184.21\", dst=\"222.240.4.204\", ttl=56)/scapy.ICMP(type=14, id=0xFF62, seq=4311)\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dd06e9-27fb-488f-a1e8-f161553cf326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de resúmenes: 73\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/Conversations/ICMP/Conv_summaries.txt\",\"r\",encoding=\"utf8\") as f:\n",
    "    Conv_summaries = f.read().splitlines()\n",
    "\n",
    "print(\"Número de resúmenes: \" +str(len(Conv_summaries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100fea14-4b67-461c-aa8d-b55967a439bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class code_response():\n",
    "    \"Stores name and place pairs\"\n",
    "    def __init__(self, name, place):\n",
    "        self.prompt_summary = name\n",
    "        self.completion = place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0bc510-5c51-420f-b1cd-3cffab4f9848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previous responses: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    responses = pickle.load(open(\"./data/Conversations/ICMP/pickle/ICMP_pairs_Aday.pkl\", \"rb\" ))\n",
    "except:\n",
    "    responses = []\n",
    "    \n",
    "print(\"Number of previous responses: \" + str(len(responses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb631ea6-9b5d-4807-860e-3c2a804f0f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivangm/pytorch_env/pytorch_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "rag_chain = prompt | mistral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552ba376-e01b-4c6f-abba-8045236cf244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4129cbc371084097a800621a6577a0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n",
      "\n",
      "pkt1=scapy.IP(src=\"132.178.87.167\", dst=\"211.239.188.225\", ttl=DL)\\\n",
      "/scapy.ICMP(type=13, id=0x3E7A, seq=4981)\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "pkt2=scapy.IP(src=\"211.239.188.225\", dst=\"132.178.87.167\", ttl=DL)\\\n",
      "/scapy.ICMP(type=14, id=0x3E7A, seq=4981)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n",
      "\n",
      "pkt1=scapy.IP(src=\"110.221.190.216\", dst=\"195.11.171.134\", ttl=DL)\\\n",
      "/scapy.ICMP(type=13, id=0x5E2A, seq=1569)\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "pkt2=scapy.IP(src=\"195.11.171.134\", dst=\"110.221.190.216\", ttl=DL)\\\n",
      "/scapy.ICMP(type=14, id=0x5E2A, seq=1569)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n",
      "\n",
      "pkt1=scapy.IP(src=\"39.89.198.223\", dst=\"171.109.55.123\", ttl=DL)\\\n",
      "/scapy.ICMP(type=13, id=0x8B7D, seq=2416)\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "pkt2=scapy.IP(src=\"171.109.55.123\", dst=\"39.89.198.223\", ttl=DL)\\\n",
      "/scapy.ICMP(type=14, id=0x8B7D, seq=2416)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of completitions done: 70\n"
     ]
    }
   ],
   "source": [
    "from scapy.all import *\n",
    "from scapy.utils import RawPcapReader, wrpcap\n",
    "import scapy.all as scapy\n",
    "\n",
    "from scapy.layers.inet import IP, ICMP\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(len(Conv_summaries)))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(Conv_summaries)):\n",
    "\n",
    "    text_sum = \"\"\n",
    "    \n",
    "    summary = Conv_summaries[i]\n",
    "    \n",
    "    #print(\"\\n...........................................\")\n",
    "    #print(\"GENERATING SCAPY COMMANDS FOR THE FOLLOWING CONVERSATIONS (iter \" +str(i+1) +\")\\n\")\n",
    "    \n",
    "    query_content = \"This is a conversation summary:\\n\"\n",
    "    query_content += summary + \"\\n\"\n",
    "    text_sum += summary + \"\\n\"\n",
    "    query_content += \"\\nGenerate python code for creating the packets in the conversation with scapy framework.Don´t give back any comments, just bare code.Dont give back any comments or imports, just bare code and commands. Dont start the resposnses wiht ```python This is a flagrant error.\\n\"\n",
    "    query_content += \"Use the following structure:\\n\"\n",
    "    query_content += \"pkt1=...\\n\"\n",
    "    query_content += \"time.sleep(abs(random.gauss(0, 0.03)))\\n\"\n",
    "    query_content += \"pkt2=...\\n\"\n",
    "\n",
    "    \n",
    "    # Query es el mensaje que le envias al modelo\n",
    "    #print(query_content)\n",
    "\n",
    "    completion = \"\"\n",
    "    completion = rag_chain.invoke({\"system\": system_message, \"user\": query_content})\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    #print(\"\\n...Generated!\")\n",
    "\n",
    "    # Filtramos ```python\n",
    "    #pos1 = completion.find(\"```python\\n\")\n",
    "    #pos2 = completion.find(\"\\n```\")\n",
    "    #print(pos1, pos2)\n",
    "    #if pos1 != -1 and pos2 != 1:\n",
    "    #    completion = completion[pos1+len(\"```python\\n\"):pos2]\n",
    "\n",
    "    # Parece que hay problemas de indentado en el código resultante ¿?\n",
    "    completion_copy = completion\n",
    "    completion = \"\"\n",
    "    for line in completion_copy.splitlines():\n",
    "        completion += line.strip()+\"\\n\"\n",
    "        \n",
    "    # Guardamos tanto la petición como la respuesta en un archivo\n",
    "    #pickle.dump(responses, open( \"./data/Conversations/DNS/pickle/DNS_pairs_Aday.pkl\", \"wb\" ) )\n",
    "    try:\n",
    "        #print(\"Ejecutando python...\")\n",
    "        exec(completion)\n",
    "        #print(pkt1)\n",
    "        #print(pkt2)\n",
    "        counter += 1\n",
    "    except:\n",
    "        print(\"Error en la ejecución\\n\")\n",
    "        print(completion)\n",
    "        #print(\":\".join(\"{:02x}\".format(ord(c)) for c in completion))\n",
    "        continue\n",
    "\n",
    "    # Guardamos tanto el resumen de los paquetes que solicitamos y lo que devuelve el modelo para estos paquetes\n",
    "    try:\n",
    "        responses.append(code_response(text_sum, completion))\n",
    "    except:\n",
    "        print(\"Error almacenando resultado del modelo\\n\")\n",
    "    \n",
    "    # Guardamos los paquetes generados en un pcap\n",
    "    with open(\"./data/Conversations/ICMP/pcap/ICMP_Conv_generated_mixtral.pcap\", \"ba+\") as f:\n",
    "        wrpcap(f, pkt1, append=True)\n",
    "    \n",
    "    with open(\"./data/Conversations/ICMP/pcap/ICMP_Conv_generated_mixtral.pcap\", \"ba+\") as f:\n",
    "        wrpcap(f, pkt2, append=True)\n",
    "\n",
    "print(\"Number of completitions done: \" + str(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4c76d1-7ee0-4da1-96e9-04c05591b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "pkt1 = scapy.IP(src=\"222.240.4.204\", dst=\"192.168.1.20\", ttl=56)/scapy.ICMP(type=13, id=0xFF62, seq=4311)\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "pkt2 = scapy.IP(src=\"192.168.1.20\", dst=\"222.240.4.204\", ttl=56)/scapy.ICMP(type=14, id=0xFF62, seq=4311)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Este es el resumen de lo que está en el struct de responses para la última petición\n",
    "print(responses[-1].prompt_summary)\n",
    "print(responses[-1].completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b0736-acf1-467a-b549-be45a5dbda76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
