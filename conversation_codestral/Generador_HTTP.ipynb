{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbffd3e-abdf-46a3-9142-3b39b15b7e7c",
   "metadata": {},
   "source": [
    "# Creación de conversaciones HTTP\n",
    "\r",
    "Con este código se va a intentar crear todos los pasos de una conversación HTTP, mediante IA generativa. Se pretende pasar una descripción de esta conversación al modelo y que este sea capaz de crear todos los pasos/paquetes necesarios para replicar una conversación con esas mismas especificacioness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b862b41-f9cb-4c58-a5e0-4e68f10aaf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoConfig,\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  GenerationConfig,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38e4a949-83a0-47ef-94e0-01b75fa95a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ac10b2-f517-4c96-b0fd-e52097e334c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f04492a-1931-43cb-a154-e0bef3edd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15582f4b-ed2e-41d8-bcff-fce1abafe5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mixtral-8x7B-Instruct-v0.1' #'mistralai/Mixtral-8x7B-Instruct-v0.1' #'mistralai/Codestral-22B-v0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdad52f1-9b4e-42c5-8666-a36576106d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3493bf7b2b4973839f60ee3a598cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, legacy=True)\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    #device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 2048\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_k = 10\n",
    "generation_config.top_p = 0.1\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "    \n",
    "# Crear LLM Chain\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    return_full_text=False,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c61011b2-b3a9-4e72-8352-5e2d6566b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"system\", \"user\"],\n",
    "    output_parser=None,\n",
    "    partial_variables={},\n",
    "    template = \"\"\"\n",
    "    [INST]\n",
    "    {system}\n",
    "    \n",
    "    {user}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "system_message = \"You are a computer network programmer. Your goal is to generate Python code to create packages with the scapy framework based on a provided conversation summary.\\n\"\n",
    "system_message = \"For example, given the following summary of the IPv4 TCP-HTTP conversation:\\n\"\n",
    "system_message += 'Source: IP=\"193.24.227.230\", port=40301, Window: 4128 // Destination: IP=\"9.9.9.9\", port=80, Window: 256 // Others: Host=\"http://ip.webernetz.net/\", Path=\"\", Code=200,  Content_Type=\"(text/html)\"\\n\\n'\n",
    "\n",
    "system_message += \"x,y = random.sample(range(1, 100), 2)\\n\"\n",
    "system_message += 'pkt1= IP(src=\"193.24.227.230\", dst=\"9.9.9.9\")/TCP(sport=40301, dport=80, flags=\"S\", seq=y, window=4128)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'y += len(pkt1[TCP].payload)+1\\n'\n",
    "system_message += 'pkt2= IP(src=\"9.9.9.9\", dst=\"193.24.227.230\")/TCP(sport=80, dport=40301, flags=\"SA\", seq=x, ack=y, window=256)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'x += len(pkt2[TCP].payload)+1\\n'\n",
    "system_message += 'pkt3= IP(src=\"193.24.227.230\", dst=\"9.9.9.9\")/TCP(sport=40301, dport=80, flags=\"A\", seq=y, ack=x, window=4128)\\n\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'y += len(pkt3[TCP].payload)\\n'\n",
    "\n",
    "system_message += 'pkt4= IP(src=\"193.24.227.230\",  dst=\"9.9.9.9\") / TCP(sport=40301, dport=80, flags=\"PA\", seq=y, ack=x, window=8192) / HTTP() / HTTPRequest(Method =\"GET\", Http_Version=\"HTTP/1.1\", Host=\"ip.webernetz.net\", Path=\"/\")\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'y += len(pkt4[TCP].payload)\\n'\n",
    "system_message += 'pkt5= IP(src=\"9.9.9.9\", dst=\"193.24.227.230\")/TCP(sport=80, dport=40301, flags=\"A\", seq=x, ack=y, window=4128)\\n\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'x += len(pkt5[TCP].payload)\\n'\n",
    "system_message += 'pkt6= IP(src=\"9.9.9.9\",  dst=\"193.24.227.230\") / TCP(sport=80, dport=40301, flags=\"PA\", seq=x, ack=y, window=8192) / HTTP() / HTTPResponse(Http_Version= \"HTTP/1.1\", Status_Code= \"200\", Reason_Phrase= \"OK\", Content_Type=\"text/html\") / \"<HTML><BODY><H!>Hello World!</H1></BODY></HTML>\"\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'x += len(pkt6[TCP].payload)\\n'\n",
    "system_message += 'pkt7= IP(src=\"193.24.227.230\", dst=\"9.9.9.9\")/TCP(sport=40301, dport=80, flags=\"A\", seq=y, ack=x, window=4128)\\n\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'y += len(pkt7[TCP].payload)\\n'\n",
    "\n",
    "system_message += 'pkt8= IP(src=\"193.24.227.230\", dst=\"9.9.9.9\")/TCP(sport=40301, dport=80, flags=\"FA\", seq=y, ack=x, window=4128)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'y += len(pkt8[TCP].payload)\\n'\n",
    "system_message += 'pkt9= IP(src=\"9.9.9.9\", dst=\"193.24.227.230\")/TCP(sport=80, dport=40301, flags=\"A\", seq=x, ack=y, window=256)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'x += len(pkt9[TCP].payload)\\n'\n",
    "system_message += 'pkt10= IP(src=\"9.9.9.9\", dst=\"193.24.227.230\")/TCP(sport=80, dport=40301, flags=\"FA\", seq=x, ack=y, window=256)\\n'\n",
    "system_message += 'time.sleep(abs(random.gauss(0, 0.03)))\\n'\n",
    "system_message += 'x += len(pkt10[TCP].payload)\\n'\n",
    "system_message += 'pkt11= IP(src=\"193.24.227.230\", dst=\"9.9.9.9\")/TCP(sport=40301, dport=80, flags=\"A\", seq=y, ack=x, window=4128)\\n\\n'\n",
    "system_message += 'y += len(pkt11[TCP].payload)\\n'\n",
    "\n",
    "system_message += \"# Add packets to list\\n\"\n",
    "system_message += \"pktlist = [pkt1, pkt2, pkt3, pkt4, pkt5, pkt6, pkt7, pkt8, pkt9, pkt10, pkt11]\"   \n",
    "system_message += \"\\n\\n\"\n",
    "\n",
    "system_message += \"Variable names in the HTTP definition are extremely important, use the exact same word. For example, use Http_Version, not http_version, use Method, not method.\\n\\n\"\n",
    "system_message += \"In the HTTP response, if the variable code != 200, don´t use the Content_Type variable\\n\"\n",
    "system_message += \"Use the same dport for HTTPRequest (dport=80) and sport for HTTPResponse (sport=80), as this means we are creating a HTTP conection and using another port like 22 might make it seem like SSH traffic\\n\\n\"\n",
    "system_message += 'Pay special attention to the sequence and ack numbers so they make sense for a TCP conversation. The must be continuous throgh the conversation. Here you have a little explanation of how they work:\\n'\n",
    "system_message += 'The Sequence Number (SEQ) marks the starting byte of each segment sent. It increments by the number of bytes in the payload of each outgoing segment, reflecting the continuous flow of data.\\n'\n",
    "system_message += 'The Acknowledgment Number (ACK) acknowledges the receipt of data. It corresponds to the next expected byte the receiver anticipates from the sender. As segments arrive at the receiver, the ACK number adjusts to reflect the highest contiguous sequence number received.\\n'\n",
    "system_message += 'So, SEQ numbers increase with outgoing data, while ACK numbers advance with incoming acknowledgments, ensuring synchronized data exchange in a TCP connection.\\n\\n'   \n",
    "\n",
    "system_message += \"Take your time (few seconds) to generate the commands and compare your proposed solution with real HTTP functioning, if anything is different, change it in order to do as real HTTP\\n\\n\"\n",
    "system_message += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dd06e9-27fb-488f-a1e8-f161553cf326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de resúmenes: 65\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/Conversations/HTTP/Conv_summaries_vfin.txt\",\"r\",encoding=\"utf8\") as f:\n",
    "    Conv_summaries = f.read().splitlines()\n",
    "\n",
    "print(\"Número de resúmenes: \" +str(len(Conv_summaries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100fea14-4b67-461c-aa8d-b55967a439bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class code_response():\n",
    "    \"Stores name and place pairs\"\n",
    "    def __init__(self, name, place):\n",
    "        self.prompt_summary = name\n",
    "        self.completion = place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0bc510-5c51-420f-b1cd-3cffab4f9848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of previous responses: 0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    responses = pickle.load(open(\"./data/Conversations/HTTP/pickle/HTTP_pairs_Aday.pkl\", \"rb\" ))\n",
    "except:\n",
    "    responses = []\n",
    "    \n",
    "print(\"Number of previous responses: \" + str(len(responses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb631ea6-9b5d-4807-860e-3c2a804f0f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivangm/pytorch_env/pytorch_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "rag_chain = prompt | mistral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "552ba376-e01b-4c6f-abba-8045236cf244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215f35442cc9416a93fb3b296c7425d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en la ejecución\n",
      "\n",
      "Number of completitions done: 23\n"
     ]
    }
   ],
   "source": [
    "from scapy.all import *\n",
    "from scapy.utils import RawPcapReader, wrpcap\n",
    "import scapy.all as scapy\n",
    "\n",
    "from scapy.layers.inet import IP, TCP\n",
    "from scapy.layers.http import *\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(len(Conv_summaries)))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(Conv_summaries)):\n",
    "\n",
    "    text_sum = \"\"\n",
    "    \n",
    "    summary = Conv_summaries[i]\n",
    "    \n",
    "    #print(\"\\n...........................................\")\n",
    "    #print(\"GENERATING SCAPY COMMANDS FOR THE FOLLOWING CONVERSATIONS (iter \" +str(i+1) +\")\\n\")\n",
    "    \n",
    "    query_content = \"Given the following summary of a conversation:\\n\"\n",
    "    query_content += summary + \"\\n\"\n",
    "    text_sum += summary + \"\\n\"\n",
    "    query_content += \"# Instructions:\\n\"\n",
    "    query_content += \"## Generate a complete python code for creating packets with scapy framework. Use the provided example structure to simulate real-world conditions, adding the len and sleep function calls between packets.\\n\"\n",
    "    query_content += \"## Take your time (a few seconds) to validate if the last code line of your code is 'pktlist = [pkt1, pkt2,...]'; If not, make the appropriate corrections to the code.\\n\"\n",
    "    query_content += \"## Don't explain the code, just generate the code block itself. PLEASE DONT start the responses with ```python. This is a flagrant error and will make the code unexecutable. You have done this last step wrong plenty of times, take your time to generate the output without ```python please.\\n\"\n",
    "\n",
    "    # Query es el mensaje que le envias al modelo\n",
    "    #print(query_content)\n",
    "\n",
    "    completion = \"\"\n",
    "    completion = rag_chain.invoke({\"system\": system_message, \"user\": query_content})\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    #print(\"\\n...Generated!\")\n",
    "\n",
    "    # Filtramos ```python\n",
    "    #pos1 = completion.find(\"```python\\n\")\n",
    "    #pos2 = completion.find(\"\\n```\")\n",
    "    #print(pos1, pos2)\n",
    "    #if pos1 != -1 and pos2 != 1:\n",
    "    #    completion = completion[pos1+len(\"```python\\n\"):pos2]\n",
    "\n",
    "    # Parece que hay problemas de indentado en el código resultante ¿?\n",
    "    completion_copy = completion\n",
    "    completion = \"\"\n",
    "    for line in completion_copy.splitlines():\n",
    "        completion += line.strip()+\"\\n\"\n",
    "        \n",
    "    # Guardamos tanto la petición como la respuesta en un archivo\n",
    "    #pickle.dump(responses, open( \"./data/Conversations/ARP/pickle/ARP_pairs_Aday.pkl\", \"wb\" ) )\n",
    "    try:\n",
    "        #print(\"Ejecutando python...\")\n",
    "        exec(completion)\n",
    "        #print(pktlist)\n",
    "        counter += 1\n",
    "    except:\n",
    "        print(\"Error en la ejecución\\n\")\n",
    "        #print(completion)\n",
    "        #print(\":\".join(\"{:02x}\".format(ord(c)) for c in completion))\n",
    "        continue\n",
    "\n",
    "    # Guardamos tanto el resumen de los paquetes que solicitamos y lo que devuelve el modelo para estos paquetes\n",
    "    try:\n",
    "        responses.append(code_response(text_sum, completion))\n",
    "    except:\n",
    "        print(\"Error almacenando resultado del modelo\\n\")\n",
    "    \n",
    "    # Guardamos los paquetes generados en un pcap\n",
    "    with open(\"./data/Conversations/HTTP/pcap/HTTP_generated_mixtral.pcap\", \"ba+\") as f:\n",
    "        wrpcap(f, pktlist, append=True)\n",
    "\n",
    "print(\"Number of completitions done: \" + str(counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab4c76d1-7ee0-4da1-96e9-04c05591b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: IP=\"192.168.0.10\", port=4592, Window: 6553 // Destination: IP=\"192.168.0.11\", port=80, Window: 3276 // Others: Host=\"http://www.bbc.co.uk/\", Path=\"\", Code=200,  Reason_Phrase= \"OK\"\n",
      "\n",
      "\n",
      "from scapy.all import *\n",
      "import time\n",
      "import random\n",
      "\n",
      "summary = {\n",
      "\"source\": {\"ip\": \"192.168.0.10\", \"port\": 4592, \"window\": 6553},\n",
      "\"destination\": {\"ip\": \"192.168.0.11\", \"port\": 80, \"window\": 3276},\n",
      "\"others\": {\"host\": \"http://www.bbc.co.uk/\", \"path\": \"\", \"code\": 200, \"reason\\_phrase\": \"OK\"}\n",
      "}\n",
      "\n",
      "x, y = random.sample(range(1, 100), 2)\n",
      "\n",
      "pkt1 = IP(src=summary[\"source\"][\"ip\"], dst=summary[\"destination\"][\"ip\"]) / TCP(sport=summary[\"source\"][\"port\"], dport=summary[\"destination\"][\"port\"], flags=\"S\", seq=y, window=summary[\"source\"][\"window\"])\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "y += len(pkt1[TCP].payload) + 1\n",
      "\n",
      "pkt2 = IP(src=summary[\"destination\"][\"ip\"], dst=summary[\"source\"][\"ip\"])/ TCP(sport=summary[\"destination\"][\"port\"], dport=summary[\"source\"][\"port\"], flags=\"SA\", seq=x, ack=y, window=summary[\"destination\"][\"window\"])\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "x += len(pkt2[TCP].payload) + 1\n",
      "\n",
      "pkt3 = IP(src=summary[\"source\"][\"ip\"], dst=summary[\"destination\"][\"ip\"])/ TCP(sport=summary[\"source\"][\"port\"], dport=summary[\"destination\"][\"port\"], flags=\"A\", seq=y, ack=x, window=summary[\"source\"][\"window\"])\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "y += len(pkt3[TCP].payload)\n",
      "\n",
      "pkt4 = IP(src=summary[\"source\"][\"ip\"], dst=summary[\"destination\"][\"ip\"])\\\n",
      "/ TCP(sport=summary[\"source\"][\"port\"], dport=summary[\"destination\"][\"port\"], flags=\"PA\", seq=y, ack=x, window=min(summary[\"source\"][\"window\"], 8192))\\\n",
      "/ HTTP() / HTTPRequest(Method=\"GET\", Http_Version=\"HTTP/1.1\", Host=summary[\"others\"][\"host\"], Path=summary[\"others\"][\"path\"])\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "y += len(pkt4[TCP].payload)\n",
      "\n",
      "pkt5 = IP(src=summary[\"destination\"][\"ip\"], dst=summary[\"source\"][\"ip\"])/ TCP(sport=summary[\"destination\"][\"port\"], dport=summary[\"source\"][\"port\"], flags=\"A\", seq=x, ack=y, window=min(summary[\"destination\"][\"window\"], 4128))\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "x += len(pkt5[TCP].payload) + 1\n",
      "\n",
      "pkt6 = IP(src=summary[\"destination\"][\"ip\"], dst=summary[\"source\"][\"ip\"])\\\n",
      "/ TCP(sport=summary[\"destination\"][\"port\"], dport=summary[\"source\"][\"port\"], flags=\"PA\", seq=x, ack=y, window=min(summary[\"destination\"][\"window\"], 8192))\\\n",
      "/ HTTP() / HTTPResponse(Http_Version=\"HTTP/1.1\", Status_Code=str(summary[\"others\"][\"code\"]), Reason_Phrase=summary[\"others\"][\"reason\\_phrase\"])\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "x += len(pkt6[TCP].payload)\n",
      "\n",
      "pkt7 = IP(src=summary[\"source\"][\"ip\"], dst=summary[\"destination\"][\"ip\"])/ TCP(sport=summary[\"source\"][\"port\"], dport=summary[\"destination\"][\"port\"], flags=\"A\", seq=y, ack=x, window=min(summary[\"source\"][\"window\"], 4128))\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "y += len(pkt7[TCP].payload)\n",
      "\n",
      "pkt8 = IP(src=summary[\"source\"][\"ip\"], dst=summary[\"destination\"][\"ip\"])/ TCP(sport=summary[\"source\"][\"port\"], dport=summary[\"destination\"][\"port\"], flags=\"FA\", seq=y, ack=x, window=min(summary[\"source\"][\"window\"], 4128))\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "y += len(pkt8[TCP].payload)\n",
      "\n",
      "pkt9 = IP(src=summary[\"destination\"][\"ip\"], dst=summary[\"source\"][\"ip\"])/ TCP(sport=summary[\"destination\"][\"port\"], dport=summary[\"source\"][\"port\"], flags=\"A\", seq=x, ack=y, window=min(summary[\"destination\"][\"window\"], 3276))\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "x += len(pkt9[TCP].payload) + 1\n",
      "\n",
      "pkt10 = IP(src=summary[\"destination\"][\"ip\"], dst=summary[\"source\"][\"ip\"])/ TCP(sport=summary[\"destination\"][\"port\"], dport=summary[\"source\"][\"port\"], flags=\"FA\", seq=x, ack=y, window=min(summary[\"destination\"][\"window\"], 3276))\n",
      "time.sleep(abs(random.gauss(0, 0.03)))\n",
      "x += len(pkt10[TCP].payload)\n",
      "\n",
      "pkt11 = IP(src=summary[\"source\"][\"ip\"], dst=summary[\"destination\"][\"ip\"])/ TCP(sport=summary[\"source\"][\"port\"], dport=summary[\"destination\"][\"port\"], flags=\"A\", seq=y, ack=x, window=min(summary[\"source\"][\"window\"], 4128))\n",
      "\n",
      "pktlist = [pkt1, pkt2, pkt3, pkt4, pkt5, pkt6, pkt7, pkt8, pkt9, pkt10, pkt11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Este es el resumen de lo que está en el struct de responses para la última petición\n",
    "print(responses[-1].prompt_summary)\n",
    "print(responses[-1].completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664f734-df0b-4eeb-9d8c-1bdd9dc3f907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
